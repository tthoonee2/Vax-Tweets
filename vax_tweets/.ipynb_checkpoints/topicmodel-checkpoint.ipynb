{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd30b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\utente\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "c:\\users\\utente\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "import spacy \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#visualization of data:\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de78b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "extension = [] #add extension\n",
    "stop.extend(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb016b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e90298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vaccination_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b58e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eliüá±üáπüá™üá∫üëå</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra üñê‚ò∫</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBio...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>Facts are immutable, Senator, even when you're...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>Explain to me again why we need a vaccine @Bor...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1337852648389832708</td>\n",
       "      <td>Dee</td>\n",
       "      <td>Birmingham, England</td>\n",
       "      <td>Gastroenterology trainee, Clinical Research Fe...</td>\n",
       "      <td>2020-01-26 21:43:12</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:11:42</td>\n",
       "      <td>Does anyone have any useful advice/guidance fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1337851215875608579</td>\n",
       "      <td>Gunther Fehlinger</td>\n",
       "      <td>Austria, Ukraine and Kosovo</td>\n",
       "      <td>End North Stream 2 now - the pipeline of corru...</td>\n",
       "      <td>2013-06-10 17:49:22</td>\n",
       "      <td>2731</td>\n",
       "      <td>5001</td>\n",
       "      <td>69344</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:06:00</td>\n",
       "      <td>it is a bit sad to claim the fame for success ...</td>\n",
       "      <td>['vaccination']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1337850832256176136</td>\n",
       "      <td>Dr.Krutika Kuppalli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID, Global Health, VHF, Pandemic Prep, Emergin...</td>\n",
       "      <td>2019-03-25 04:14:29</td>\n",
       "      <td>21924</td>\n",
       "      <td>593</td>\n",
       "      <td>7815</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:04:29</td>\n",
       "      <td>There have not been many bright days in 2020 b...</td>\n",
       "      <td>['BidenHarris', 'Election2020']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1337850023531347969</td>\n",
       "      <td>Erin Despas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Designing&amp;selling on Teespring. Like 90s Disne...</td>\n",
       "      <td>2009-10-30 17:53:54</td>\n",
       "      <td>887</td>\n",
       "      <td>1515</td>\n",
       "      <td>9639</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:01:16</td>\n",
       "      <td>Covid vaccine; You getting it?\\n\\n #CovidVacci...</td>\n",
       "      <td>['CovidVaccine', 'covid19', 'PfizerBioNTech', ...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1337842295857623042</td>\n",
       "      <td>Ch.Amjad Ali</td>\n",
       "      <td>Islamabad</td>\n",
       "      <td>#ProudPakistani #LovePakArmy #PMIK @insafiansp...</td>\n",
       "      <td>2012-11-12 04:18:12</td>\n",
       "      <td>671</td>\n",
       "      <td>2368</td>\n",
       "      <td>20469</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 19:30:33</td>\n",
       "      <td>#CovidVaccine \\n\\nStates will start getting #C...</td>\n",
       "      <td>['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name                user_location  \\\n",
       "0  1340539111971516416            Rachel Roh    La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong            San Francisco, CA   \n",
       "2  1337858199140118533              eliüá±üáπüá™üá∫üëå                     Your Bed   \n",
       "3  1337855739918835717         Charles Adler       Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                          NaN   \n",
       "5  1337852648389832708                   Dee          Birmingham, England   \n",
       "6  1337851215875608579     Gunther Fehlinger  Austria, Ukraine and Kosovo   \n",
       "7  1337850832256176136   Dr.Krutika Kuppalli                          NaN   \n",
       "8  1337850023531347969           Erin Despas                          NaN   \n",
       "9  1337842295857623042          Ch.Amjad Ali                    Islamabad   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra üñê‚ò∫  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "5  Gastroenterology trainee, Clinical Research Fe...  2020-01-26 21:43:12   \n",
       "6  End North Stream 2 now - the pipeline of corru...  2013-06-10 17:49:22   \n",
       "7  ID, Global Health, VHF, Pandemic Prep, Emergin...  2019-03-25 04:14:29   \n",
       "8  Designing&selling on Teespring. Like 90s Disne...  2009-10-30 17:53:54   \n",
       "9  #ProudPakistani #LovePakArmy #PMIK @insafiansp...  2012-11-12 04:18:12   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "5             105           108              106          False   \n",
       "6            2731          5001            69344          False   \n",
       "7           21924           593             7815           True   \n",
       "8             887          1515             9639          False   \n",
       "9             671          2368            20469          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  Same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  While the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  #coronavirus #SputnikV #AstraZeneca #PfizerBio...   \n",
       "3  2020-12-12 20:23:59  Facts are immutable, Senator, even when you're...   \n",
       "4  2020-12-12 20:17:19  Explain to me again why we need a vaccine @Bor...   \n",
       "5  2020-12-12 20:11:42  Does anyone have any useful advice/guidance fo...   \n",
       "6  2020-12-12 20:06:00  it is a bit sad to claim the fame for success ...   \n",
       "7  2020-12-12 20:04:29  There have not been many bright days in 2020 b...   \n",
       "8  2020-12-12 20:01:16  Covid vaccine; You getting it?\\n\\n #CovidVacci...   \n",
       "9  2020-12-12 19:30:33  #CovidVaccine \\n\\nStates will start getting #C...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "5                                                NaN   Twitter for iPhone   \n",
       "6                                    ['vaccination']      Twitter Web App   \n",
       "7                    ['BidenHarris', 'Election2020']   Twitter for iPhone   \n",
       "8  ['CovidVaccine', 'covid19', 'PfizerBioNTech', ...      Twitter Web App   \n",
       "9  ['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...      Twitter Web App   \n",
       "\n",
       "   retweets  favorites  is_retweet  \n",
       "0         0          0       False  \n",
       "1         1          1       False  \n",
       "2         0          0       False  \n",
       "3       446       2129       False  \n",
       "4         0          0       False  \n",
       "5         0          0       False  \n",
       "6         0          4       False  \n",
       "7         2         22       False  \n",
       "8         2          1       False  \n",
       "9         0          0       False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a7e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Same folks said daikon paste could treat a cytokine storm #PfizerBioNTech https://t.co/xeHhIMg1kF', \"While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we've ev‚Ä¶ https://t.co/dlCHrZjkhm\", '#coronavirus #SputnikV #AstraZeneca #PfizerBioNTech #Moderna #Covid_19 Russian vaccine is created to last 2-4 years‚Ä¶ https://t.co/ieYlCKBr8P', \"Facts are immutable, Senator, even when you're not ethically sturdy enough to acknowledge them. (1) You were born i‚Ä¶ https://t.co/jqgV18kch4\", 'Explain to me again why we need a vaccine @BorisJohnson @MattHancock #whereareallthesickpeople #PfizerBioNTech‚Ä¶ https://t.co/KxbSRoBEHq', 'Does anyone have any useful advice/guidance for whether the COVID vaccine is safe whilst breastfeeding?‚Ä¶ https://t.co/EifsyQoeKN', 'it is a bit sad to claim the fame for success of #vaccination on patriotic competition between USA, Canada, UK and‚Ä¶ https://t.co/IfMrAyGyTP', 'There have not been many bright days in 2020 but here are some of the best \\n1. #BidenHarris winning #Election2020‚Ä¶ https://t.co/77u4f8XXfx', 'Covid vaccine; You getting it?\\n\\n #CovidVaccine #covid19 #PfizerBioNTech #Moderna', '#CovidVaccine \\n\\nStates will start getting #COVID19Vaccine Monday, #US says \\n#pakustv #NYC #Healthcare #GlobalGoals‚Ä¶ https://t.co/MksOvBvs5w']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = df['text']\n",
    "s_list  = s.to_list()\n",
    "s_str = ' '.join(s)\n",
    "print(s_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03161c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tqdm{} Progress Bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9879/9879 [00:00<00:00, 50692.92it/s]\n"
     ]
    }
   ],
   "source": [
    "s_list2 = []\n",
    "for p in tqdm(s_list, desc = 'tqdm{} Progress Bar'):\n",
    "    t =  re.sub(r\"\\S*https?:\\S*\", \"\", p)\n",
    "    s_list2.append(t)\n",
    "s_list = s_list2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f4fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tqdm{} Progress Bar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9879/9879 [01:13<00:00, 134.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in tqdm(texts, desc = 'tqdm{} Progress Bar'):\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(s_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fab7df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['same folk say daikon paste treat cytokine storm # pfizerbiontech', \"world have be wrong side history year hopefully big vaccination effort 've ev\", 'coronavirus sputnikv astrazeneca # PfizerBioNTech russian vaccine be create last year', 'fact be immutable even when be ethically sturdy enough acknowledge be bear', 'explain again why need vaccine @matthancock whereareallthesickpeople # pfizerbiontech', 'do have useful advice guidance vaccine be safe breastfeed', 'be bit sad claim fame success vaccination patriotic competition', 'have be many bright day here be good win # election2020', 'vaccine get covid19 pfizerbiontech', 'state start get covid19vaccine say # pakustv #']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8c4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['same', 'folk', 'say', 'daikon', 'paste']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "print (data_words[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019b55c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlist = [] --> this was my previous idea for the cleaning of the words given the stopword list\\nnsent = []\\nfor firstlist in tqdm(data_words):\\n    for word in firstlist:\\n         if word not in stop:\\n                nsent.append(word)\\n    nlist.append(nsent)\\nprint(nlist)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nlist = [] --> this was my previous idea for the cleaning of the words given the stopword list\n",
    "nsent = []\n",
    "for firstlist in tqdm(data_words):\n",
    "    for word in firstlist:\n",
    "         if word not in stop:\n",
    "                nsent.append(word)\n",
    "    nlist.append(nsent)\n",
    "print(nlist)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee545fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams and trigrams based on occurance within the dataset\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# Remove Stop Words --> insanity function\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop] for doc in texts] #rewrite\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams) #equal to the followig line\n",
    "data_bi_tri = trigram_mod[data_words_bigrams] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b68d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_bi_tri)\n",
    "texts = data_bi_tri\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c52e771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tqdm{} Progress Bar:   0%|                                                                    | 0/9879 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4612/1760297748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4612/1908838373.py\u001b[0m in \u001b[0;36mlemmatization\u001b[1;34m(texts, allowed_postags)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tqdm{} Progress Bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mnew_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utente\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \"\"\"\n\u001b[1;32m--> 988\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utente\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1068\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             )\n\u001b[1;32m-> 1070\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     def update(\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"corpus = [] \n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "    \n",
    "print(corpus)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da891ef",
   "metadata": {},
   "source": [
    "the result is: (index in dict , frequency of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef30fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = id2word[[0][:1][0]] #retrieving a word from this mess\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2119ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word,num_topics=10,update_every=1,passes=10,alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb74fa",
   "metadata": {},
   "source": [
    "<H1>ANALYSIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90c2a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word,mds=\"mmds\",R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfed68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
